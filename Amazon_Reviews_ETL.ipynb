{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V58rxea0HqSa",
    "outputId": "e8227b2e-f570-4c4e-c0a0-5ceab2a993c7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
      "\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [920 kB]\n",
      "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
      "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
      "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
      "Hit:12 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,021 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,313 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,544 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,017 kB]\n",
      "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,398 kB]\n",
      "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,136 kB]\n",
      "Fetched 12.7 MB in 6s (1,968 kB/s)\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Find the latest version of spark 3.2 from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.2.3'\n",
    "spark_version = \"spark-3.2.3\"\n",
    "os.environ[\"SPARK_VERSION\"] = spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xKwTpATHqSe",
    "outputId": "0526e971-8181-40c3-ca7f-f9c9a01198dd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2023-03-18 16:51:27--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n",
      "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
      "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1002883 (979K) [application/java-archive]\n",
      "Saving to: ‘postgresql-42.2.16.jar’\n",
      "\n",
      "postgresql-42.2.16. 100%[===================>] 979.38K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2023-03-18 16:51:27 (11.4 MB/s) - ‘postgresql-42.2.16.jar’ saved [1002883/1002883]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the Postgres driver that will allow Spark to interact with Postgres.\n",
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MMqDAjVS0KN9"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"M17-Amazon-Challenge\")\n",
    "    .config(\"spark.driver.extraClassPath\", \"/content/postgresql-42.2.16.jar\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyBsySGuY-9V"
   },
   "source": [
    "### Load Amazon Data into Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CtCmBhQJY-9Z",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "00887eb6-9956-4c59-ccf9-9a5c8b464fb1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "|         US|   12039526| RTIS3L2M1F5SM|B001CXYMFS|     737716809|Thrustmaster T-Fl...|     Video Games|          5|            0|          0|   N|                Y|an amazing joysti...|Used this for Eli...| 2015-08-31|\n",
      "|         US|    9636577| R1ZV7R40OLHKD|B00M920ND6|     569686175|Tonsee 6 buttons ...|     Video Games|          5|            0|          0|   N|                Y|Definitely a sile...|Loved it,  I didn...| 2015-08-31|\n",
      "|         US|    2331478|R3BH071QLH8QMC|B0029CSOD2|      98937668|Hidden Mysteries:...|     Video Games|          1|            0|          1|   N|                Y|            One Star|poor quality work...| 2015-08-31|\n",
      "|         US|   52495923|R127K9NTSXA2YH|B00GOOSV98|      23143350|GelTabz Performan...|     Video Games|          3|            0|          0|   N|                Y|good, but could b...|nice, but tend to...| 2015-08-31|\n",
      "|         US|   14533949|R32ZWUXDJPW27Q|B00Y074JOM|     821342511|Zero Suit Samus a...|     Video Games|          4|            0|          0|   N|                Y|   Great but flawed.|Great amiibo, gre...| 2015-08-31|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Video_Games_v1_00.tsv.gz\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.option(\"encoding\", \"UTF-8\").csv(\n",
    "    SparkFiles.get(\"\"), sep=\"\\t\", header=True, inferSchema=True\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yUSe55VY-9t"
   },
   "source": [
    "### Create DataFrames to match tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8REmY1aY-9u"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Read in the Review dataset as a DataFrame\n",
    "# Spark data drame wwas read into dataframe variable called df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "B0TESUDRY-90",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d9989f9c-86a4-4c24-c48b-09052f89de49"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|customer_count|\n",
      "+-----------+--------------+\n",
      "|   48670265|             1|\n",
      "|   49103216|             2|\n",
      "|    1131200|             1|\n",
      "|   43076447|             2|\n",
      "|   46261368|             1|\n",
      "+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Create the customers_table DataFrame\n",
    "customers_df = (\n",
    "    df.groupby(\"customer_id\")\n",
    "    .agg(count(\"customer_id\"))\n",
    "    .withColumnRenamed(\"count(customer_id)\", \"customer_count\")\n",
    ")\n",
    "\n",
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4FwXA6UvY-96",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "56282522-72ae-42d1-ee82-d8a2f3632a0b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+--------------------+\n",
      "|product_id|       product_title|\n",
      "+----------+--------------------+\n",
      "|B00CJ7IUI6|The Elder Scrolls...|\n",
      "|B00DHF39KS|Wolfenstein: The ...|\n",
      "|B00MUTAVH6|Under Night In-Bi...|\n",
      "|B001AZSEUW|              Peggle|\n",
      "|B00KVOVBGM|PlayStation 4 Con...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the products_table DataFrame and drop duplicates.\n",
    "products_df = df.select([\"product_id\", \"product_title\"]).drop_duplicates()\n",
    "\n",
    "products_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MkqyCuNQY-9-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "210795de-2f07-4f21-aacf-6bb9e1728187"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+-----------+----------+--------------+-----------+\n",
      "|     review_id|customer_id|product_id|product_parent|review_date|\n",
      "+--------------+-----------+----------+--------------+-----------+\n",
      "| RTIS3L2M1F5SM|   12039526|B001CXYMFS|     737716809| 2015-08-31|\n",
      "| R1ZV7R40OLHKD|    9636577|B00M920ND6|     569686175| 2015-08-31|\n",
      "|R3BH071QLH8QMC|    2331478|B0029CSOD2|      98937668| 2015-08-31|\n",
      "|R127K9NTSXA2YH|   52495923|B00GOOSV98|      23143350| 2015-08-31|\n",
      "|R32ZWUXDJPW27Q|   14533949|B00Y074JOM|     821342511| 2015-08-31|\n",
      "+--------------+-----------+----------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Create the review_id_table DataFrame.\n",
    "# Convert the 'review_date' column to a date datatype with to_date(\"review_date\", 'yyyy-MM-dd').alias(\"review_date\")\n",
    "review_id_df = df.select(\n",
    "    [\n",
    "        \"review_id\",\n",
    "        \"customer_id\",\n",
    "        \"product_id\",\n",
    "        \"product_parent\",\n",
    "        to_date(\"review_date\", \"yyyy-MM-dd\").alias(\"review_date\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "review_id_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lzMmkdKmY--D",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c27dba38-1dc1-4671-dd49-e37a9f841784"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+-----------+-------------+-----------+----+-----------------+\n",
      "|     review_id|star_rating|helpful_votes|total_votes|vine|verified_purchase|\n",
      "+--------------+-----------+-------------+-----------+----+-----------------+\n",
      "| RTIS3L2M1F5SM|          5|            0|          0|   N|                Y|\n",
      "| R1ZV7R40OLHKD|          5|            0|          0|   N|                Y|\n",
      "|R3BH071QLH8QMC|          1|            0|          1|   N|                Y|\n",
      "|R127K9NTSXA2YH|          3|            0|          0|   N|                Y|\n",
      "|R32ZWUXDJPW27Q|          4|            0|          0|   N|                Y|\n",
      "+--------------+-----------+-------------+-----------+----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the vine_table. DataFrame\n",
    "vine_df = df.select(\n",
    "    [\n",
    "        \"review_id\",\n",
    "        \"star_rating\",\n",
    "        \"helpful_votes\",\n",
    "        \"total_votes\",\n",
    "        \"vine\",\n",
    "        \"verified_purchase\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "vine_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jITZhLkmY--J"
   },
   "source": [
    "### Connect to the AWS RDS instance and write each DataFrame to its table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7jiUvs1aY--L"
   },
   "outputs": [],
   "source": [
    "# Configure settings for RDS\n",
    "mode = \"append\"\n",
    "jdbc_url = (\n",
    "    \"jdbc:postgresql://dataviz.ctb2jb8ouakf.us-east-1.rds.amazonaws.com:5432/postgres\"\n",
    ")\n",
    "config = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"lZRVIgqJRept5z8ggNKo\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "T2zgZ-aKY--Q"
   },
   "outputs": [],
   "source": [
    "# Write review_id_df to table in RDS\n",
    "review_id_df.write.jdbc(\n",
    "    url=jdbc_url, table=\"review_id_table\", mode=mode, properties=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "1m3yzn-LY--U"
   },
   "outputs": [],
   "source": [
    "# Write products_df to table in RDS\n",
    "# about 3 min\n",
    "products_df.write.jdbc(\n",
    "    url=jdbc_url, table=\"products_table\", mode=mode, properties=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "KbXri15fY--Z"
   },
   "outputs": [],
   "source": [
    "# Write customers_df to table in RDS\n",
    "# 5 min 14 s\n",
    "customers_df.write.jdbc(\n",
    "    url=jdbc_url, table=\"customers_table\", mode=mode, properties=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "XdQknSHLY--e"
   },
   "outputs": [],
   "source": [
    "# Write vine_df to table in RDS\n",
    "# 11 minutes\n",
    "vine_df.write.jdbc(url=jdbc_url, table=\"vine_table\", mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Exuo6ebUsCqW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}